# awesome-multimodal-representation-learning

## Multimodal Representation Learning: Recent Works on V+L Pretraining Methods
Curated list of papers and resources that leverage different modalities --e.g., image and text--to learn better conceptual representations.  
 
## Transformer-based Pretraining
- **ROSITA: Enhancing Vision-and-Language Semantic Alignments via Cross-and Intra-modal Knowledge Integration**, (2021) [[paper]](https://dl.acm.org/doi/pdf/10.1145/3474085.3475251), by *Yuhao Cui, Zhou Yu, Chunqi Wang, Zhongzhou Zhao, Ji Zhang, Meng Wang, and Jun Yu*
- **Less is More: ClipBERT for Video-and-Language Learning via Sparse Sampling**, (2021) [[paper]](https://openaccess.thecvf.com/content/CVPR2021/papers/Lei_Less_Is_More_ClipBERT_for_Video-and-Language_Learning_via_Sparse_Sampling_CVPR_2021_paper.pdf), by *Jie Lei, Linjie Li, Luowei Zhou, Zhe Gan, Tamara L. Berg, Mohit Bansal, and Jingjing Liu*
- **Transformer is All You Need: Multimodal Multitask Learning with a Unified Transformer**, (2021) [[paper]](https://arxiv.org/pdf/2102.10772.pdf), by *Ronghang Hu and Amanpreet Singh*
- **Oscar: Object-Semantics Aligned Pre-training for Vision-Language Tasks**, (2020) [[paper]](https://arxiv.org/pdf/2004.06165.pdf), by *Xiujun Li, Xi Yin, Chunyuan Li, Pengchuan Zhang, Xiaowei Hu, Lei Zhang, Lijuan Wang, Houdong Hu, Li Dong, Furu Wei, Yejin Choi, and Jianfeng Gao*
- **Uniter: Universal Image-text Representation Learning**, (2020) [[paper]](https://arxiv.org/pdf/1909.11740.pdf), by *Yen-Chun Chen, Linjie Li, Licheng Yu, Ahmed El Kholy, Faisal Ahmed, Zhe Gan, Yu Cheng, and Jingjing Liu.*

## Contrastive Learning Across Modalities
- **Learning Transferable Visual Models From Natural Language Supervision**, (2020) [[paper]](https://arxiv.org/pdf/2103.00020.pdf), by *Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever* 

## Recent Papers on Video Dataset
- **VideoBERT: A Joint Model for Video and Language Representation Learning** (2019) [[paper]](https://openaccess.thecvf.com/content_ICCV_2019/papers/Sun_VideoBERT_A_Joint_Model_for_Video_and_Language_Representation_Learning_ICCV_2019_paper.pdf), by *Chen Sun, Austin Myers, Carl Vondrick, Kevin Murphy, and Cordelia Schmid*
- **Video Description: A Survey of Methods, Datasets and Evaluation Metrics** (2019) [[paper]](https://dl.acm.org/doi/pdf/10.1145/3355390), by *Nayyer Aafaq, Ajmal Mian, Wei Liu, Syed Zulqarnain Gilani, and Mubarak Shah*
- **Video Paragraph Captioning Using Hierarchical Recurrent Neural Networks** (2016) [[paper]](https://openaccess.thecvf.com/content_cvpr_2016/papers/Yu_Video_Paragraph_Captioning_CVPR_2016_paper.pdf), by *Haonan Yu, Jiang Wang, Zhiheng Huang, Yi Yang, and Wei Xu*

## Useful Blogs and Videos
- **OpenAI Multimodal Research** [[website]](https://openai.com/blog/tags/multimodal/)
  - [[Blog--Multimodal Neurons in Artificial Neural Networks]](https://openai.com/blog/multimodal-neurons/), by *Gabriel Goh, Chelsea Voss, Daniela Amodei, Shan Carter, Michael Petrov, Justin Jay Wang, Nick Cammarata, and Chris Olah*, March 2021
  - [[Blog--CLIP: Connecting Text and Images]](https://openai.com/blog/clip/), by *Alec Radford, Ilya Sutskever, Jong Wook Kim, Gretchen Krueger, and Sandhini Agarwal*, Jan 2021
  - [[Blog--DALLÂ·E: Creating Images from Text]](https://openai.com/blog/dall-e/), by *(primary authors) Aditya RameshMikhail PavlovGabriel GohScott Gray, (supporting authors) Mark Chen, Rewon Child, Vedant Misra, Pamela Mishkin, Gretchen Krueger, Sandhini Agarwal, and Ilya Sutskever*, Jan 2021
  - [[Blog--Simple Implementation of OpenAI CLIP model: A Tutorial]](https://towardsdatascience.com/simple-implementation-of-openai-clip-model-a-tutorial-ace6ff01d9f2), by *Moein Shariatnia*, April 2021
- **Microsoft Multimodal AI** [[website]](https://multimodalai.azurewebsites.net/)
  - [[CVPR 2020 Tutorial--Recent Advances in Vision-and-Language Research]](https://rohit497.github.io/Recent-Advances-in-Vision-and-Language-Research/), June 2020
    - [[Youtube--CVPR 2020 Tutorial: Talk #1]](https://www.youtube.com/watch?v=BJgzjNN3h1U) Opening Remarks by JJ Liu
    - [[Youtube--CVPR 2020 Tutorial: Talk #2]](https://www.youtube.com/watch?v=n4mUriUrYR0) Visual QA and Reasoning by Zhe Gan
    - [[Youtube--CVPR 2020 Tutorial: Talk #3]](https://www.youtube.com/watch?v=Zn5uFGsq4j4) Visual Captioning by Luowei Zhou
    - [[Youtube--CVPR 2020 Tutorial: Talk #4]](https://www.youtube.com/watch?v=8pNtrsFe4tk) Text-to-Image Generation by Yu Cheng
    - [[Youtube--CVPR 2020 Tutorial: Talk #5]](https://www.youtube.com/watch?v=C4UQWJcp7w4) Self-supervised Learning by Licheng Yu, Yen-Chun Chen and Linjie Li
  - [[Blog--UNITER: Combining Image and Text]](https://towardsdatascience.com/uniter-d979e2d838f0), by *"Rohit Pillai*, May 2020
  - [[Blog--VIOLIN: Do you really understand videos?]](https://medium.com/swlh/violin-do-you-really-understand-videos-fe07f7affd9e), by *"Rohit Pillai*, May 2020
  - [[Youtube--AI Advances in Image Captioning: Describing Images as Well as People Do]](https://www.youtube.com/watch?v=QNesnXfyYq8), by *Lijuan Wang and Xiaowei Hu*, Mar 2021

- **Grounded Video Description** 
  - [[Youtube--Grounded Video Description]](https://openaccess.thecvf.com/content_cvpr_2016/papers/Yu_Video_Paragraph_Captioning_CVPR_2016_paper.pdf)
  - [[Youtube--ActivityNet Workshop: Dense-Captioning Events in Videos]](https://www.youtube.com/watch?v=jpgv_vmGG58)
